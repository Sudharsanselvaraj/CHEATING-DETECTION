# -*- coding: utf-8 -*-
"""gaze_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Od89A4Gk5FNkGACzBKrtYntVrlrV6p4
"""

import streamlit as st
import cv2
import numpy as np
from tensorflow.keras.models import load_model
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

# Load the trained gaze model
model = load_model("gaze_model.h5")

# Class label mapping
labels_map = {0: "down", 1: "front", 2: "left", 3: "right"}
IMG_SIZE = (64, 64)

# Video stream processor
class GazeDetector(VideoTransformerBase):
    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape
        cx, cy = w // 2, h // 2
        crop = img[cy - 64:cy + 64, cx - 64:cx + 64]

        if crop.shape[0] == 128 and crop.shape[1] == 128:
            eye = cv2.resize(crop, IMG_SIZE)
            eye = eye.astype("float32") / 255.0
            eye = np.expand_dims(eye, axis=0)
            pred = model.predict(eye)
            label = labels_map[np.argmax(pred)]
            cv2.putText(img, f"Gaze: {label}", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)
            cv2.rectangle(img, (cx - 64, cy - 64), (cx + 64, cy + 64), (255, 0, 0), 2)

        return img

# Streamlit UI
st.set_page_config(page_title="Gaze Detection", layout="centered")
st.title("üëÅÔ∏è Real-Time Gaze Direction Detection")
webrtc_streamer(key="gaze", video_transformer_factory=GazeDetector)